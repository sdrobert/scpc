num_epochs: 1000
log10_learning_rate: -4 # Initial optimizer log-learning rate. If unspecified, the initial learning rate of the optimizer instance remains unchanged
early_stopping_threshold: 0.001 # Minimum magnitude decrease in validation metric from the last best that resets the early stopping clock. If zero, early stopping will never be performed
early_stopping_patience: 10 # Number of epochs after which, if the classifier has failed to decrease its validation metric by a threshold, training is halted
early_stopping_burnin: 20 # Number of epochs before the early stopping criterion kicks in
# reduce_lr_threshold: 0.0 # Minimum magnitude decrease in validation metric from the last best that resets the clock for reducing the learning rate. If zero, the learning rate will never be reduced
# reduce_lr_factor: 0.1 # Factor by which to multiply the learning rate if there has been no improvement in the  after "reduce_lr_patience" epochs
# reduce_lr_patience: 1 # Number of epochs after which, if the classifier has failed to decrease its validation metric by a threshold, the learning rate is reduced
# reduce_lr_cooldown: 0 # Number of epochs after reducing the learning rate before we resume checking improvements
# reduce_lr_log10_epsilon: -8 # The log10 absolute difference between learning rates that, below which, reducing the learning rate is considered meaningless
# reduce_lr_burnin: 0 # Number of epochs before the criterion for reducing the learning rate kicks in
seed: # Seed used for training procedures (e.g. dropout). If unset, will not touch torch's seeding
keep_last_and_best_only: true # If the model is being saved, keep only the model and optimizer parameters for the last and best epoch (in terms of validation loss). If False, save every epoch. See also "saved_model_fmt" and "saved_optimizer_fmt"
saved_model_fmt: model_{epoch:03d}.pt # The file name format string used to save model state information. Entries from the state csv are used to format this string (see TrainingStateController)
saved_optimizer_fmt: optim_{epoch:03d}.pt # The file name format string used to save optimizer state information. Entries from the state csv are used to format this string (see TrainingStateController)
optimizer: adamw # Which optimizer to train with
dropout: 0.1 # Dropout probability
spec_aug_max_time_warp: 80.0 # SpecAugment: maximum frames warping can shift
spec_aug_max_freq_warp: 0.0 # SpecAugment: maximum feat coeffs warping can shift
spec_aug_max_time_mask: 100 # SpecAugment: absolute max number of consecutive frames to mask
spec_aug_max_freq_mask: ${input_size} # SpecAugment: absolute max number of consecutive feat coeffs to mask
spec_aug_max_time_mask_proportion: 0.04 # SpecAugment: relative max number of consecutive frames to mask
spec_aug_max_freq_mask_proportion: 0.04 # SpecAugment: relative max number of consecutive feat coeffs to mask
spec_aug_num_time_mask: 20 # SpecAugment: absolute max number of frame masks to apply
spec_aug_num_time_mask_proportion: 0.04 # SpecAgument: relative max number of frame masks to apply
spec_aug_num_freq_mask: 2 # SpecAugment: absolute max number of feat coeff masks to apply
spec_aug_interpolation_order: 1 # SpecAugment: warping interpolation order

## disable dropout
# dropout: 0.0

# ## disable SpecAugment
# spec_aug_max_time_warp: 0.0 # SpecAugment: maximum frames warping can shift
# spec_aug_max_freq_warp: 0.0 # SpecAugment: maximum feat coeffs warping can shift
# spec_aug_max_time_mask: 0 # SpecAugment: absolute max number of consecutive frames to mask
# spec_aug_max_freq_mask: 0 # SpecAugment: absolute max number of consecutive feat coeffs to mask
