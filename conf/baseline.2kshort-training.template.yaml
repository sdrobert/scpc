num_epochs: 1000
log10_learning_rate: -3 # Initial optimizer log-learning rate. If unspecified, the initial learning rate of the optimizer instance remains unchanged
early_stopping_threshold: 0.001 # Minimum magnitude decrease in validation metric from the last best that resets the early stopping clock. If zero, early stopping will never be performed
early_stopping_patience: 10 # Number of epochs after which, if the classifier has failed to decrease its validation metric by a threshold, training is halted
early_stopping_burnin: 20 # Number of epochs before the early stopping criterion kicks in
seed: # Seed used for training procedures (e.g. dropout). If unset, will not touch torch's seeding
keep_last_and_best_only: true # If the model is being saved, keep only the model and optimizer parameters for the last and best epoch (in terms of validation loss). If False, save every epoch. See also "saved_model_fmt" and "saved_optimizer_fmt"
optimizer: adamw # Which optimizer to train with
saved_model_fmt: model_{epoch:03d}.pt # The file name format string used to save model state information. Entries from the state csv are used to format this string (see TrainingStateController)
saved_optimizer_fmt: optim_{epoch:03d}.pt # The file name format string used to save optimizer state information. Entries from the state csv are used to format this string (see TrainingStateController)
dropout: 0.0
spec_aug_max_time_warp: 0.0 # SpecAugment: maximum frames warping can shift
spec_aug_max_freq_warp: 0.0 # SpecAugment: maximum feat coeffs warping can shift
spec_aug_max_time_mask: 0 # SpecAugment: absolute max number of consecutive frames to mask
spec_aug_max_freq_mask: 0 # SpecAugment: absolute max number of consecutive feat coeffs to mask
